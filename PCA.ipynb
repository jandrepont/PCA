{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis \n",
    "Using Jon Shlens \"A Tutorial On Principal Component Analysis\" (2003). \n",
    "## 1) Change of Basis\n",
    "\"Is there another basis, which is a linear combination of the original basis, that best expresses our data set?\" \n",
    "Let $\\bf{X}$ and $\\bf{Y}$ be $m\\times n$ matrices related by a linear transformation $\\bf{P}$. $\\bf{X}$ is the original recorded data set and $\\bf{Y}$ is a re-representation of that data set \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\bf{PX} = \\bf{Y}\n",
    "\\end{equation}\n",
    "$$\n",
    "We define the following quantities : \n",
    "\n",
    "* $\\bf{p_i}$ are the rows of $\\bf{P}$\n",
    "* $\\bf{x_i}$ are the columns of $\\bf{X}$\n",
    "* $\\bf{y_i}$ are the columns of $\\bf{Y}$\n",
    "\n",
    "Then each $\\bf{y_i}$ has the form \n",
    "$$\\bf{y_i} =  \\begin{bmatrix}\n",
    "                \\langle \\bf{p_1, x_i}\\rangle \\\\\n",
    "                \\vdots \\\\ \n",
    "                \\langle \\bf{p_m, x_i}\\rangle \\\\\n",
    "            \\end{bmatrix}$$\n",
    "\n",
    "that is, the jth coefficient of $\\bf{y_i}$ is a projection on to the jth row of $\\bf{P}$. \n",
    "In fact, we can interpret the above form of $\\bf{Y}$ as a projection onto the basis $\\{\\bf{p_1}, \\dots, \\bf{p_m} \\}$ \n",
    "\n",
    "We now come to the question of choosing a good $\\bf{P}$. \n",
    "\n",
    "## 2) Choosing P\n",
    "We want a data set that has low levels of noise and low redundancy. \n",
    "### Noise \n",
    "A common measure for noise is the signal to noise ratio, SNR : \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\sigma^2_{\\text{signal}}}{\\sigma^2_{\\text{noise}}}\n",
    "\\end{equation}\n",
    "$$\n",
    "how this is actually measured is specific to the data and measurement devices. \n",
    "\n",
    "### Redundancy \n",
    "In short, we do not want dimensions that tell the same story, i.e. high correlation. \n",
    "\n",
    "The SNR and redundancy can be determined using the covariance matrix. \n",
    "\n",
    "## Covariance Matrix\n",
    "Consider two random variables $a,b$ each with $\\mu=0$, then the measure of their \n",
    "covariance is given by \n",
    "$$\n",
    "(\\mu_a - a)(\\mu_b - b) = ab\n",
    "$$\n",
    "likewise, we can extend this to random vectors with zero mean, \n",
    "$$\n",
    "\\sigma^2_{ab} \\equiv \\frac{1}{n-1}\\bf{ab}^T \n",
    "$$\n",
    "\n",
    "then for our data matrix $\\bf{X}$ the covariance matrix $\\bf{S_X}$ is \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\bf{S_X} \\equiv \\frac{1}{n-1}\\bf{XX}^T\n",
    "\\end{equation}\n",
    "$$\n",
    "### Properties \n",
    "* $\\bf{S_X}$ is a square symmetric $m \\times m$ matrix\n",
    "* The diagonal terms of $\\bf{S_X}$ are the variance of the particular measurement types\n",
    "* Off diagonal terms are the covariance between measurement types. \n",
    "\n",
    "### Optimizing $\\bf{S_X}$\n",
    "\n",
    "Suppose we want to optimize certain properties of $\\bf{S_X}$, such as redundancy and SNR. \n",
    "What might this look like? We will refer to the manipulated $\\bf{S_X}$ as $\\bf{S_y}$. \n",
    "\n",
    "We begin with redundancy. \n",
    "\n",
    "#### Redundancy \n",
    "Obviously, the off diagonal terms would be 0, in that they are orthogonal. Therefore, \n",
    "removing redundancy diagonalizes $\\bf{S_Y}$\n",
    "PCA assumes \n",
    "1. that all the basis vectors, $\\bf{P}$ are orthonormal explained by argument above\n",
    "2. The directions with largest variances are the most important. This can be seen when considering how one might even build a an orthonormal matrix, given that we need directions perpindicular to all previous selected directions, which would fail for small magnitudes. \n",
    "\n",
    "## 3) Summary Of Assumptions and Limits\n",
    "\n",
    "1. Linearity\n",
    "\n",
    "2. Mean and variance are sufficient statistics \n",
    "\n",
    "3. Large variances have important dynamics \n",
    "\n",
    "4. The principal components are orthogonal\n",
    "\n",
    "## 4) Solving PCA : Eigenvectors of Covariance \n",
    "We summarize our goal : \n",
    "\n",
    "Find some orthonormal matrix $\\bf{P}$ where $\\bf{Y} = \\bf{PX}$ such that $\\bf{S_Y} \\equiv \n",
    "\\frac{1}{n-1}\\bf{YY}^T$ is diagonalized. The rows of $\\bf{P}$ are the principal components\n",
    "of $\\bf{X}$. \n",
    "\n",
    "Right now we have two unkowns. \n",
    "$\n",
    "\\begin{align*}\n",
    "\\bf{S_Y} &= \\frac{1}{n-1}\\bf{YY}^T \\\\\n",
    "    &= \\frac{1}{n-1}(\\bf{PX})(\\bf{PX})^T \\\\\n",
    "    &= \\frac{1}{n-1}\\bf{PX}\\bf{X}^T\\bf{P}^T \\\\\n",
    "\\end{align*}\n",
    "$\n",
    "We define $\\bf{A} = \\bf{XX}^T$, which is symmetric. \n",
    "\n",
    "\\begin{theorem}\n",
    "A matrix is symmetric if and only if it is orthogonally diagonalizable.\n",
    "\\end{theorem}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
